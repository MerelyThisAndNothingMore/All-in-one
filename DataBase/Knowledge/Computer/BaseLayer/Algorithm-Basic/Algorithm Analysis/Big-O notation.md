#computer #algorithm
# Introduction
Big-O notation is a mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity.
In computer science, big-O notation is used to classify algorithms
according to how their run time or space requirement grow as the order of approximation. 
f(N) is O(g(N)) if for some C and N0 the following condition holds:
$$\forall N>N_{0}; f(N)<C g(N)$$
https://www.topcoder.com/thrive/articles/Computational%20Complexity%20part%20one