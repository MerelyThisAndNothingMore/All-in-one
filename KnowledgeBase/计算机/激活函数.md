---
tags: 
aliases:
  - Activation Function
---

# 定义

激活函数（Activation Function）是[[神经网络]]中每个神经元的输出函数，用于引入非线性特性。激活函数通过将神经元的加权和输入到函数中，决定神经元的输出值。它们帮助神经网络学习和表示复杂的非线性关系。

## 作用

1. **引入非线性**：使得神经网络可以表示和学习非线性函数，提高模型的表达能力。
2. **限制输出范围**：将神经元的输出值限制在一个特定范围内，防止输出值过大或过小。
3. **加速收敛**：通过适当的激活函数，可以加速神经网络的训练过程，提高训练效率。

# 常见的激活函数

### 1. Sigmoid 函数

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

**特点**：
- 输出范围：$(0, 1)$
- 常用于二分类问题的输出层
- 存在梯度消失问题，不适合深层网络

### 2. Tanh 函数

$$
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

**特点**：
- 输出范围：$(-1, 1)$
- 相较于 Sigmoid，Tanh 函数的输出均值为 0，收敛速度更快
- 仍存在梯度消失问题

### 3. ReLU 函数

$$
\text{ReLU}(x) = \max(0, x)
$$

**特点**：
- 输出范围： \[$0,\infty$\]
- 简单且计算高效，适用于大多数隐藏层
- 解决了梯度消失问题，但存在“死亡 ReLU”问题，即部分神经元可能永远不会被激活

### 4. Leaky ReLU 函数

$$
\text{Leaky ReLU}(x) = 
\begin{cases} 
x & \text{if } x > 0 \\
\alpha x & \text{if } x \leq 0
\end{cases}
$$

**特点**：
- 输出范围：$(-\infty, \infty)$
- 解决了“死亡 ReLU”问题，允许负值的输入通过较小的斜率 $\alpha$ 输出

### 5. Softmax 函数

$$
\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}
$$

**特点**：
- 输出范围：$(0, 1)$，且所有输出值的和为 1
- 常用于多分类问题的输出层
- 将输出转化为概率分布

## 使用场景

1. **Sigmoid 和 Tanh**：常用于网络的输出层，特别是二分类问题。
2. **ReLU 和 Leaky ReLU**：常用于网络的隐藏层，适用于大多数深层神经网络。
3. **Softmax**：用于多分类问题的输出层，将输出转化为概率分布。

## 示例：Python 实现激活函数

以下是使用 Python 实现常见激活函数的示例：

```python
import numpy as np

# Sigmoid 函数
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Tanh 函数
def tanh(x):
    return np.tanh(x)

# ReLU 函数
def relu(x):
    return np.maximum(0, x)

# Leaky ReLU 函数
def leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x, alpha * x)

# Softmax 函数
def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=0)

# 示例使用
x = np.array([-2, -1, 0, 1, 2])
print("Sigmoid:", sigmoid(x))
print("Tanh:", tanh(x))
print("ReLU:", relu(x))
print("Leaky ReLU:", leaky_relu(x))
print("Softmax:", softmax(x))
```

# Q & A

**Q1: 为什么需要激活函数？**

A1: 激活函数引入非线性特性，使神经网络能够表示和学习复杂的非线性关系。如果没有激活函数，神经网络只能表示线性变换，无法处理复杂的数据。

**Q2: 如何选择合适的激活函数？**

A2: 选择激活函数通常基于具体问题和网络结构：
- 对于输出层，二分类问题常用 Sigmoid，多分类问题常用 Softmax。
- 对于隐藏层，ReLU 是默认选择，因为其计算高效且效果良好。Leaky ReLU 可以解决 ReLU 的“死亡”问题。
- Tanh 在某些情况下表现比 Sigmoid 更好，但仍可能存在梯度消失问题。

**Q3: 什么是梯度消失问题，如何缓解？**

A3: 梯度消失问题是指在[[反向传播]]过程中，梯度逐渐变小，导致权重更新缓慢甚至停止。缓解方法包括使用 ReLU 或其变体作为激活函数、使用更好的权重初始化方法和批量归一化等技术。

**Q4: 什么是 ReLU 的“死亡”问题？**

A4: ReLU 的“死亡”问题是指当 ReLU 神经元的输入值始终为负时，该神经元的输出恒为 0，从而无法更新其权重。解决方法包括使用 Leaky ReLU、Parametric ReLU 或其他变体激活函数。

**Q5: 激活函数如何影响神经网络的训练速度和性能？**

A5: 激活函数影响梯度的传播和权重的更新速度，从而影响训练速度和性能。合适的激活函数可以加快收敛速度，提高模型的表达能力和泛化性能。选择不当的激活函数可能导致梯度消失、梯度爆炸或其他训练问题。